{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_spark_session(app_name=\"MovieLensALS\"):\n",
    "    \"\"\"Create and return a Spark session with optimized memory configuration\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.default.parallelism\", \"100\") \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "        .config(\"spark.rdd.compress\", \"true\") \\\n",
    "        .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "        .config(\"LogLevel\", \"ERROR\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def load_ratings_data(spark, filepath):\n",
    "    \"\"\"Load and preprocess ratings data with caching\"\"\"\n",
    "    ratings_df = spark.read.csv(\n",
    "        filepath,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    ).repartition(100)  # Increased partitions\n",
    "    \n",
    "    # Convert columns to appropriate types and cache the result\n",
    "    processed_df = ratings_df.select(\n",
    "        col(\"userId\").cast(\"integer\"),\n",
    "        col(\"movieId\").cast(\"integer\"),\n",
    "        col(\"rating\").cast(\"float\")\n",
    "    ).cache()  # Cache the DataFrame\n",
    "    \n",
    "    # Force cache computation\n",
    "    processed_df.count()\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def train_als_model(training_data, validation_data):\n",
    "    \"\"\"Train ALS model with cross-validation for hyperparameter tuning\"\"\"\n",
    "    \n",
    "    # Initialize ALS model\n",
    "    als = ALS(\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        nonnegative=True,\n",
    "        coldStartStrategy=\"drop\",\n",
    "        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n",
    "        finalStorageLevel=\"MEMORY_AND_DISK\"\n",
    "    )\n",
    "    \n",
    "    # Create parameter grid for cross-validation\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(als.rank, [10,20]) \\\n",
    "        .addGrid(als.maxIter, [5,15]) \\\n",
    "        .addGrid(als.regParam, [0.01,0.1]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Define evaluator\n",
    "    evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    \n",
    "    # Create CrossValidator\n",
    "    cv = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=2,\n",
    "        parallelism=2\n",
    "    )\n",
    "    \n",
    "    # Fit the model using cross-validation\n",
    "    print(\"Starting model training...\")\n",
    "    cv_model = cv.fit(training_data)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    predictions = best_model.transform(validation_data)\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Print best parameters and validation RMSE\n",
    "    print(\"\\nBest Model Parameters:\")\n",
    "    # print(f\"Rank: {best_model.getRank()}\")\n",
    "    # print(f\"MaxIter: {best_model.getMaxIter()}\")\n",
    "    # print(f\"RegParam: {best_model.getRegParam()}\")\n",
    "    print(f\"Rank: {best_model._java_obj.parent().getRank()}\")\n",
    "    print(f\"MaxIter: {best_model._java_obj.parent().getMaxIter()}\")\n",
    "    print(f\"RegParam: {best_model._java_obj.parent().getRegParam()}\")\n",
    "    print(f\"Validation RMSE: {rmse}\")\n",
    "    \n",
    "    return best_model, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: LogLevel\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/22 22:39:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 19999186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data count: 5000909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: float]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ratings data\n",
    "ratings_df = load_ratings_data(spark, \"data/ratings.csv\")\n",
    "\n",
    "# # Take a smaller sample for initial testing\n",
    "# sampled_df = ratings_df.sample(fraction=0.1, seed=42)\n",
    "\n",
    "# Split data into training (80%) and validation (20%) sets\n",
    "training_data, validation_data = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Training data count:\", training_data.count())\n",
    "print(\"Validation data count:\", validation_data.count())\n",
    "\n",
    " # Cache the datasets\n",
    "training_data.cache()\n",
    "validation_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/22 22:40:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model Parameters:\n",
      "Rank: 20\n",
      "MaxIter: 15\n",
      "RegParam: 0.1\n",
      "Validation RMSE: 0.8071295973063569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training completed and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train model and get validation results\n",
    "best_model, validation_rmse = train_als_model(training_data, validation_data)\n",
    "\n",
    "# Save the model for later use\n",
    "# best_model.save(\"models/best_als_model\")\n",
    "best_model.write().overwrite().save(\"models/best_als_model\")\n",
    "\n",
    "print(\"\\nModel training completed and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist cached DataFrames\n",
    "if 'training_data' in locals():\n",
    "    training_data.unpersist()\n",
    "if 'validation_data' in locals():\n",
    "    validation_data.unpersist()\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "#     # Create Spark session\n",
    "#     spark = create_spark_session()\n",
    "    \n",
    "#     try:\n",
    "#         # Load ratings data\n",
    "#         ratings_df = load_ratings_data(spark, \"data/ratings.csv\")\n",
    "        \n",
    "#         # Split data into training (80%) and validation (20%) sets\n",
    "#         training_data, validation_data = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "        \n",
    "#         print(\"Training data count:\", training_data.count())\n",
    "#         print(\"Validation data count:\", validation_data.count())\n",
    "        \n",
    "#         # Train model and get validation results\n",
    "#         best_model, validation_rmse = train_als_model(training_data, validation_data)\n",
    "        \n",
    "#         # Save the model for later use\n",
    "#         best_model.save(\"models/best_als_model\")\n",
    "        \n",
    "#         print(\"\\nModel training completed and saved successfully!\")\n",
    "        \n",
    "#     finally:\n",
    "#         # Stop Spark session\n",
    "#         spark.stop()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
