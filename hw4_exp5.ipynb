{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: LogLevel\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/02 14:53:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/02 14:53:37 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 19999186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data count: 5000909\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 14:54:45 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model Parameters:\n",
      "Rank: 10\n",
      "MaxIter: 12\n",
      "RegParam: 0.15\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 0.8377\n",
      "MSE: 0.7017\n",
      "MAP@10: 0.2383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training completed and saved successfully!\n",
      "\n",
      "Training Hybrid Model...\n",
      "\n",
      "Training Hybrid Model...\n",
      "\n",
      "Trying ALS weight: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:07:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:08:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:09:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:10:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:11:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:12:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:13:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:14:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:15:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:16:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:17:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:18:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:19:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:20:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:21:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:22:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:22:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:22:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:22:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:22:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:23:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:24:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:26:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:27:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/02 15:36:56 WARN CacheManager: Asked to cache already cached data.        \n",
      "24/12/02 15:36:56 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying ALS weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 15:38:48 WARN CacheManager: Asked to cache already cached data.        \n",
      "24/12/02 15:38:48 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying ALS weight: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Enhanced Hybrid Model...\n",
      "\n",
      "Training Enhanced Hybrid Model...\n",
      "\n",
      "Trying weights: ALS=0.4, CF=0.3, Supervised=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/atewary/anaconda3/envs/MMD/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.4.1.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/12/02 15:53:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1846\n",
      "MSE: 1.4032\n",
      "MAP@10: 0.2369\n",
      "\n",
      "Trying weights: ALS=0.6, CF=0.2, Supervised=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3412:(56 + 8) / 100][Stage 3414:>(0 + 0) / 33][Stage 3416:>(0 + 0) / 16] \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, udf, collect_list, struct, avg, count, sum\n",
    "from pyspark.sql.types import FloatType, ArrayType, StructType, StructField\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import col, split, expr, broadcast\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import count, expr, collect_list, col, sqrt, when, lit, rank, split, explode, sum as sql_sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import numpy as np\n",
    "\n",
    "def create_spark_session(app_name=\"MovieLensALS\"):\n",
    "    \"\"\"Create and return a Spark session with optimized memory configuration\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.local.dir\", \"/scratch/atewary/\") \\\n",
    "        .config(\"spark.driver.memory\", \"12g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"8g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.default.parallelism\", \"100\") \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "        .config(\"spark.rdd.compress\", \"true\") \\\n",
    "        .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.6\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Xss4m\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Xss4m\") \\\n",
    "        .config(\"LogLevel\", \"ERROR\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def load_ratings_data(spark, filepath):\n",
    "    \"\"\"Load and preprocess ratings data with caching\"\"\"\n",
    "    ratings_df = spark.read.csv(\n",
    "        filepath,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    ).repartition(100)  # Increased partitions\n",
    "    \n",
    "    # Convert columns to appropriate types and cache the result\n",
    "    processed_df = ratings_df.select(\n",
    "        col(\"userId\").cast(\"integer\"),\n",
    "        col(\"movieId\").cast(\"integer\"),\n",
    "        col(\"rating\").cast(\"float\")\n",
    "    ).cache()  # Cache the DataFrame\n",
    "    \n",
    "    # Force cache computation\n",
    "    processed_df.count()\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def calculate_map(predictions_df, k=10):\n",
    "    \"\"\"Calculate Mean Average Precision at K\"\"\"\n",
    "    \n",
    "    # Create window spec for ranking predictions per user\n",
    "    window = Window.partitionBy(\"userId\").orderBy(F.col(\"prediction\").desc())\n",
    "    \n",
    "    # Add rank and filter top K predictions per user\n",
    "    predictions_with_rank = predictions_df.withColumn(\n",
    "        \"rank\", F.row_number().over(window)\n",
    "    ).filter(F.col(\"rank\") <= k)\n",
    "    \n",
    "    # Calculate precision at each position for each user\n",
    "    predictions_with_precision = predictions_with_rank.withColumn(\n",
    "        \"precision\", \n",
    "        F.when(F.col(\"rating\") >= 4.0, 1.0/F.col(\"rank\")).otherwise(0.0)\n",
    "    )\n",
    "    \n",
    "    # Calculate cumulative precision (AP) for each user\n",
    "    user_ap = predictions_with_precision.groupBy(\"userId\").agg(\n",
    "        F.sum(\"precision\").alias(\"ap\"),\n",
    "        F.count(\"precision\").alias(\"num_predictions\")\n",
    "    )\n",
    "    \n",
    "    # Calculate MAP\n",
    "    map_value = user_ap.select(\n",
    "        (F.sum(\"ap\") / F.sum(\"num_predictions\")).alias(\"map\")\n",
    "    ).first()[\"map\"]\n",
    "    \n",
    "    return map_value\n",
    "\n",
    "def evaluate_metrics(predictions_df):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    \n",
    "    # RMSE\n",
    "    rmse_evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    rmse = rmse_evaluator.evaluate(predictions_df)\n",
    "    \n",
    "    # MSE\n",
    "    mse_evaluator = RegressionEvaluator(\n",
    "        metricName=\"mse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    mse = mse_evaluator.evaluate(predictions_df)\n",
    "    \n",
    "    # MAP\n",
    "    map_score = calculate_map(predictions_df)\n",
    "    \n",
    "    return {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MSE\": mse,\n",
    "        \"MAP@10\": map_score\n",
    "    }\n",
    "\n",
    "def train_als_model(training_data, validation_data):\n",
    "    \"\"\"Train ALS model with cross-validation for hyperparameter tuning\"\"\"\n",
    "    \n",
    "    # Initialize ALS model\n",
    "    als = ALS(\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        nonnegative=True,\n",
    "        implicitPrefs=False,\n",
    "        coldStartStrategy=\"drop\",\n",
    "        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n",
    "        finalStorageLevel=\"MEMORY_AND_DISK\"\n",
    "    )\n",
    "    \n",
    "    # Create parameter grid for cross-validation\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(als.rank, [5,10]) \\\n",
    "        .addGrid(als.maxIter, [6,12]) \\\n",
    "        .addGrid(als.regParam, [0.15]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Define evaluator\n",
    "    evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    \n",
    "    # Create CrossValidator\n",
    "    cv = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3,\n",
    "        parallelism=2\n",
    "    )\n",
    "    \n",
    "    # Fit the model using cross-validation\n",
    "    print(\"Starting model training...\")\n",
    "    cv_model = cv.fit(training_data)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    predictions = best_model.transform(validation_data)\n",
    "    metrics = evaluate_metrics(predictions)\n",
    "    \n",
    "    # Print best parameters and validation RMSE\n",
    "    print(\"\\nBest Model Parameters:\")\n",
    "    print(f\"Rank: {best_model._java_obj.parent().getRank()}\")\n",
    "    print(f\"MaxIter: {best_model._java_obj.parent().getMaxIter()}\")\n",
    "    print(f\"RegParam: {best_model._java_obj.parent().getRegParam()}\")\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
    "    print(f\"MSE: {metrics['MSE']:.4f}\")\n",
    "    print(f\"MAP@10: {metrics['MAP@10']:.4f}\")\n",
    "    \n",
    "    return best_model, metrics\n",
    "\n",
    "\n",
    "class ItemItemCF:\n",
    "    \"\"\"Item-based Collaborative Filtering component with optimized implementation\"\"\"\n",
    "    def __init__(self, k_neighbors=5):  # Reduced from 10 to 5 neighbors\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.item_similarities = None\n",
    "        \n",
    "    def fit(self, ratings_df):\n",
    "        \"\"\"\n",
    "        Compute item-item similarities using Pearson correlation\n",
    "        Optimized implementation with reduced shuffling and better caching\n",
    "        \"\"\"\n",
    "        # Calculate mean rating for each movie and broadcast\n",
    "        mean_ratings = ratings_df.groupBy(\"movieId\").agg(\n",
    "            F.avg(\"rating\").alias(\"mean_rating\")\n",
    "        ).cache()\n",
    "        \n",
    "        # Normalize ratings in one efficient step with broadcast join\n",
    "        normalized_ratings = ratings_df.join(\n",
    "            broadcast(mean_ratings), \n",
    "            \"movieId\"\n",
    "        ).withColumn(\n",
    "            \"norm_rating\", \n",
    "            F.col(\"rating\") - F.col(\"mean_rating\")\n",
    "        ).select(\n",
    "            \"userId\", \n",
    "            \"movieId\", \n",
    "            \"norm_rating\"\n",
    "        )\n",
    "        \n",
    "        # Calculate similarities with reduced shuffling\n",
    "        self.item_similarities = normalized_ratings.alias(\"r1\").join(\n",
    "            normalized_ratings.alias(\"r2\"),\n",
    "            (F.col(\"r1.userId\") == F.col(\"r2.userId\")),  # Join on userId\n",
    "            \"inner\"\n",
    "        ).where(\n",
    "            F.col(\"r1.movieId\") < F.col(\"r2.movieId\")  # Ensure unique pairs\n",
    "        ).groupBy(\n",
    "            F.col(\"r1.movieId\"),\n",
    "            F.col(\"r2.movieId\")\n",
    "        ).agg(\n",
    "            F.count(\"*\").alias(\"common_users\"),\n",
    "            F.sum(F.col(\"r1.norm_rating\") * F.col(\"r2.norm_rating\")).alias(\"dot_product\"),\n",
    "            F.sum(F.pow(F.col(\"r1.norm_rating\"), 2)).alias(\"norm1_squared\"),\n",
    "            F.sum(F.pow(F.col(\"r2.norm_rating\"), 2)).alias(\"norm2_squared\")\n",
    "        ).filter(\n",
    "            F.col(\"common_users\") >= 5\n",
    "        ).select(\n",
    "            F.col(\"r1.movieId\").alias(\"movieId\"),\n",
    "            F.col(\"r2.movieId\").alias(\"movieId2\"),\n",
    "            (F.col(\"dot_product\") / (F.sqrt(F.col(\"norm1_squared\")) * \n",
    "                                    F.sqrt(F.col(\"norm2_squared\")))).alias(\"similarity\")\n",
    "        ).cache()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, test_data):\n",
    "        \"\"\"\n",
    "        Transform method that generates predictions efficiently\n",
    "        Replaces the individual predict method with batch processing\n",
    "        \"\"\"\n",
    "        # Create window spec for top-K similar items\n",
    "        window = Window.partitionBy(\"movieId\").orderBy(F.col(\"similarity\").desc())\n",
    "        \n",
    "        # Get top-K similar items for each movie\n",
    "        top_similarities = self.item_similarities.withColumn(\n",
    "            \"rank\", \n",
    "            F.row_number().over(window)\n",
    "        ).filter(\n",
    "            F.col(\"rank\") <= self.k_neighbors\n",
    "        )\n",
    "        \n",
    "        # Join with test data and calculate predictions - using aliases\n",
    "        predictions = test_data.alias(\"test\").join(\n",
    "            broadcast(top_similarities).alias(\"sim\"),\n",
    "            F.col(\"test.movieId\") == F.col(\"sim.movieId\"),\n",
    "            \"left_outer\"\n",
    "        ).join(\n",
    "            test_data.select(\n",
    "                F.col(\"userId\"),\n",
    "                F.col(\"movieId\").alias(\"movieId2\"),\n",
    "                F.col(\"rating\")\n",
    "            ).alias(\"ratings2\"),\n",
    "            [\"userId\", \"movieId2\"],\n",
    "            \"left_outer\"\n",
    "        ).groupBy(\n",
    "            F.col(\"test.userId\"),\n",
    "            F.col(\"test.movieId\")\n",
    "        ).agg(\n",
    "            (F.sum(F.col(\"similarity\") * F.col(\"ratings2.rating\")) / \n",
    "            F.sum(F.abs(F.col(\"similarity\")))).alias(\"prediction\")\n",
    "        ).na.fill(0)\n",
    "        \n",
    "        return predictions.select(\n",
    "            F.col(\"userId\"),\n",
    "            F.col(\"movieId\"),\n",
    "            F.col(\"prediction\")\n",
    "        )\n",
    "class HybridRecommender:\n",
    "    def __init__(self, als_model, weight_als=0.7):\n",
    "        self.als_model = als_model\n",
    "        self.item_cf = ItemItemCF(k_neighbors=5)  # Reduced neighbors\n",
    "        self.weight_als = weight_als\n",
    "        self.weight_cf = 1.0 - weight_als\n",
    "    \n",
    "    def fit(self, training_data):\n",
    "        self.item_cf.fit(training_data)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, test_data):\n",
    "        # Get predictions from both models\n",
    "        als_predictions = self.als_model.transform(test_data)\n",
    "        cf_predictions = self.item_cf.transform(test_data)\n",
    "        \n",
    "        # Combine predictions efficiently\n",
    "        combined = als_predictions.join(\n",
    "            cf_predictions.select(\n",
    "                \"userId\", \"movieId\",\n",
    "                F.col(\"prediction\").alias(\"cf_prediction\")\n",
    "            ),\n",
    "            [\"userId\", \"movieId\"],\n",
    "            \"left_outer\"\n",
    "        ).fillna(0, subset=['cf_prediction'])\n",
    "        \n",
    "        # Calculate weighted prediction\n",
    "        return combined.withColumn(\n",
    "            \"prediction\",\n",
    "            (self.weight_als * F.col(\"prediction\") + \n",
    "             self.weight_cf * F.col(\"cf_prediction\"))\n",
    "        ).select(\"userId\", \"movieId\", \"prediction\", \"rating\")\n",
    "\n",
    "def train_hybrid_model(als_model, training_data, validation_data):\n",
    "    \"\"\"Simplified hybrid model training with fewer weight combinations\"\"\"\n",
    "    print(\"\\nTraining Hybrid Model...\")\n",
    "    \n",
    "    # Try fewer weight combinations\n",
    "    weight_combinations = [(0.3, 0.7), (0.5, 0.5), (0.7, 0.3)]\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    \n",
    "    for als_weight in [w[0] for w in weight_combinations]:\n",
    "        print(f\"\\nTrying ALS weight: {als_weight}\")\n",
    "        \n",
    "        hybrid_model = HybridRecommender(\n",
    "            als_model=als_model,\n",
    "            weight_als=als_weight\n",
    "        ).fit(training_data)\n",
    "        \n",
    "        predictions = hybrid_model.transform(validation_data)\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = hybrid_model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "class SupervisedPredictor:\n",
    "    def __init__(self, max_depth=5, num_trees=10):\n",
    "        self.model = None\n",
    "        self.max_depth = max_depth\n",
    "        self.num_trees = num_trees\n",
    "        \n",
    "    def prepare_features(self, ratings_df, movies_df):\n",
    "        # 1. Extract user statistics with null handling\n",
    "        user_stats = ratings_df.groupBy(\"userId\").agg(\n",
    "            F.coalesce(F.avg(\"rating\"), F.lit(0.0)).alias(\"user_avg_rating\"),\n",
    "            F.coalesce(F.stddev(\"rating\"), F.lit(0.0)).alias(\"user_rating_std\"),\n",
    "            F.count(\"rating\").cast(\"double\").alias(\"user_rating_count_double\")\n",
    "        ).cache()\n",
    "        \n",
    "        # 2. Extract movie statistics with null handling\n",
    "        movie_stats = ratings_df.groupBy(\"movieId\").agg(\n",
    "            F.coalesce(F.avg(\"rating\"), F.lit(0.0)).alias(\"movie_avg_rating\"),\n",
    "            F.coalesce(F.stddev(\"rating\"), F.lit(0.0)).alias(\"movie_rating_std\"),\n",
    "            F.count(\"rating\").cast(\"double\").alias(\"movie_rating_count_double\")\n",
    "        ).cache()\n",
    "        \n",
    "        # 3. Extract and process genres\n",
    "        genres_df = movies_df.select(\n",
    "            \"movieId\",\n",
    "            *[\n",
    "                F.when(\n",
    "                    F.array_contains(F.split(\"genres\", \"\\\\|\"), genre), \n",
    "                    F.lit(1.0)\n",
    "                ).otherwise(F.lit(0.0)).alias(genre)\n",
    "                for genre in [\n",
    "                    \"Action\", \"Adventure\", \"Animation\", \"Children\", \n",
    "                    \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\",\n",
    "                    \"Film-Noir\", \"Horror\", \"IMAX\", \"Musical\", \"Mystery\", \n",
    "                    \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "                ]\n",
    "            ]\n",
    "        ).cache()\n",
    "        \n",
    "        # 3. Combine all features\n",
    "        enriched_data = ratings_df \\\n",
    "            .join(broadcast(user_stats), \"userId\") \\\n",
    "            .join(broadcast(movie_stats), \"movieId\") \\\n",
    "            .join(broadcast(genres_df), \"movieId\")\n",
    "        \n",
    "        # 4. Create feature vector\n",
    "        feature_cols = [col for col in enriched_data.columns if col not in \n",
    "                    [\"userId\", \"movieId\", \"rating\"]]\n",
    "        \n",
    "        feature_assembler = VectorAssembler(\n",
    "            inputCols=feature_cols,\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        # 5. Prepare final dataset\n",
    "        final_data = feature_assembler.transform(enriched_data) \\\n",
    "            .select(\"userId\", \"movieId\", \"rating\", \"features\")\n",
    "        \n",
    "        # Handle null values\n",
    "        final_data = final_data.na.fill(0)\n",
    "        # Unpersist cached DataFrames\n",
    "        user_stats.unpersist()\n",
    "        movie_stats.unpersist()\n",
    "        genres_df.unpersist()\n",
    "        \n",
    "        return final_data\n",
    "    \n",
    "    def fit(self, ratings_df, movies_df):\n",
    "        # Prepare training data\n",
    "        training_data = self.prepare_features(ratings_df, movies_df)\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        rf = RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"rating\",\n",
    "            maxDepth=self.max_depth,\n",
    "            numTrees=self.num_trees,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        self.model = rf.fit(training_data)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, ratings_df, movies_df):\n",
    "        # Prepare test data and make predictions\n",
    "        test_data = self.prepare_features(ratings_df, movies_df)\n",
    "        return self.model.transform(test_data)\n",
    "\n",
    "class EnhancedHybridRecommender:\n",
    "    def __init__(self, hybrid_model, als_weight=0.4, item_cf_weight=0.3, supervised_weight=0.3):\n",
    "        self.hybrid_model = hybrid_model  # Store the hybrid model\n",
    "        self.als_weight = als_weight\n",
    "        self.item_cf_weight = item_cf_weight\n",
    "        self.supervised_weight = supervised_weight\n",
    "        self.supervised_model = None\n",
    "    \n",
    "    def fit(self, training_data, movies_df):\n",
    "        # Train only the supervised component since hybrid model is already trained\n",
    "        self.supervised_model = SupervisedPredictor().fit(training_data, movies_df)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, test_data, movies_df):\n",
    "        # Get predictions from hybrid model (this includes both ALS and Item-CF)\n",
    "        hybrid_predictions = self.hybrid_model.transform(test_data)\n",
    "        \n",
    "        # Get predictions from supervised model\n",
    "        supervised_predictions = self.supervised_model.transform(test_data, movies_df)\n",
    "        \n",
    "        # Combine predictions\n",
    "        combined = hybrid_predictions.join(\n",
    "            supervised_predictions.select(\n",
    "                \"userId\", \n",
    "                \"movieId\",\n",
    "                col(\"prediction\").alias(\"supervised_prediction\")\n",
    "            ),\n",
    "            [\"userId\", \"movieId\"],\n",
    "            \"outer\"\n",
    "        ).na.fill(0)\n",
    "        \n",
    "        # Calculate weighted prediction\n",
    "        # Note: hybrid_predictions.prediction already contains combined ALS and Item-CF predictions\n",
    "        final_predictions = combined.withColumn(\n",
    "            \"prediction\",\n",
    "            (col(\"prediction\") * (self.als_weight + self.item_cf_weight) + \n",
    "             col(\"supervised_prediction\") * self.supervised_weight)\n",
    "        ).select(\"userId\", \"movieId\", \"prediction\", \"rating\")\n",
    "        \n",
    "        return final_predictions\n",
    "\n",
    "def train_enhanced_hybrid_model(training_data, validation_data, movies_df, hybrid_model):\n",
    "    \"\"\"Train enhanced hybrid model with all three components\"\"\"\n",
    "    print(\"\\nTraining Enhanced Hybrid Model...\")\n",
    "\n",
    "    weight_combinations = [\n",
    "        (0.4, 0.3, 0.3),  # Equal-ish weights\n",
    "        (0.6, 0.2, 0.2),  # ALS dominant\n",
    "        (0.2, 0.6, 0.2),  # Item-CF dominant\n",
    "        (0.2, 0.2, 0.6)   # Supervised dominant\n",
    "    ]\n",
    "\n",
    "    weight_performances = []\n",
    "    for als_w, cf_w, sup_w in weight_combinations:\n",
    "        print(f\"\\nTrying weights: ALS={als_w}, CF={cf_w}, Supervised={sup_w}\")\n",
    "        \n",
    "        try:\n",
    "            enhanced_model = EnhancedHybridRecommender(\n",
    "                hybrid_model=hybrid_model,  # Pass the entire hybrid model\n",
    "                als_weight=als_w,\n",
    "                item_cf_weight=cf_w,\n",
    "                supervised_weight=sup_w\n",
    "            )\n",
    "            \n",
    "            # Train model (only trains the supervised component)\n",
    "            enhanced_model.fit(training_data, movies_df)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            predictions = enhanced_model.transform(validation_data, movies_df)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = evaluate_metrics(predictions)\n",
    "            weight_performances.append(((als_w, cf_w, sup_w), metrics))\n",
    "            \n",
    "            print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
    "            print(f\"MSE: {metrics['MSE']:.4f}\")\n",
    "            print(f\"MAP@10: {metrics['MAP@10']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with weights {(als_w, cf_w, sup_w)}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if weight_performances:\n",
    "        best_weights, best_metrics = min(weight_performances, key=lambda x: x[1]['RMSE'])\n",
    "        print(\"\\nBest Configuration:\")\n",
    "        print(f\"Weights (ALS, CF, Supervised): {best_weights}\")\n",
    "        print(f\"RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "        \n",
    "        return EnhancedHybridRecommender(\n",
    "            hybrid_model=hybrid_model,\n",
    "            als_weight=best_weights[0],\n",
    "            item_cf_weight=best_weights[1],\n",
    "            supervised_weight=best_weights[2]\n",
    "        ).fit(training_data, movies_df)\n",
    "    else:\n",
    "        raise Exception(\"No valid models were trained\")\n",
    "\n",
    "# Create Spark session\n",
    "spark = create_spark_session()\n",
    "# Load ratings data\n",
    "ratings_df = load_ratings_data(spark, \"data/ratings.csv\")\n",
    "\n",
    "# Take a smaller sample for initial testing\n",
    "# sampled_df = ratings_df.sample(fraction=0.00001, seed=42)\n",
    "#-------------------------------------------------------\n",
    "# # Take only 2000 rows\n",
    "# ratings_df_800 = ratings_df.limit(1300).cache()\n",
    "# # Get unique movieIds from the sampled ratings\n",
    "# sampled_movie_ids = ratings_df_800.select(\"movieId\").distinct()\n",
    "\n",
    "# # Load movies data and filter to only include movies in our ratings sample\n",
    "# movies_df_full = spark.read.csv(\n",
    "#     \"data/movies.csv\",\n",
    "#     header=True,\n",
    "#     inferSchema=True\n",
    "# )\n",
    "# movies_df = movies_df_full.join(\n",
    "#     sampled_movie_ids,\n",
    "#     \"movieId\",\n",
    "#     \"inner\"\n",
    "# ).cache()\n",
    "#-------------------------------------------------------\n",
    "training_data, validation_data = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Training data count:\", training_data.count())\n",
    "print(\"Validation data count:\", validation_data.count())\n",
    "\n",
    "\n",
    "# Train model and get validation results\n",
    "als_model, als_metrics = train_als_model(training_data, validation_data)\n",
    "\n",
    "# Save the model for later use\n",
    "# best_model.save(\"models/best_als_model\")\n",
    "als_model.write().overwrite().save(\"models/best_als_model\")\n",
    "\n",
    "print(\"\\nModel training completed and saved successfully!\")\n",
    "# Add this to your main code after training the ALS model\n",
    "print(\"\\nTraining Hybrid Model...\")\n",
    "hybrid_model = train_hybrid_model(als_model=als_model,training_data=training_data, validation_data=validation_data)\n",
    "\n",
    "# Get metrics from hybrid model predictions\n",
    "two_comp_predictions = hybrid_model.transform(validation_data)\n",
    "two_comp_metrics = evaluate_metrics(two_comp_predictions)\n",
    "# Add to your main code:\n",
    "movies_df = spark.read.csv(\n",
    "    \"data/movies.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# After training three-component hybrid\n",
    "print(\"\\nTraining Enhanced Hybrid Model...\")\n",
    "enhanced_hybrid_model = train_enhanced_hybrid_model(training_data, validation_data, movies_df,hybrid_model)\n",
    "# Get predictions and evaluate final model\n",
    "final_predictions = enhanced_hybrid_model.transform(validation_data, movies_df)\n",
    "three_comp_metrics = evaluate_metrics(final_predictions)\n",
    "print(\"\\nFinal Enhanced Hybrid Model Performance:\")\n",
    "print(f\"RMSE: {three_comp_metrics['RMSE']:.4f}\")\n",
    "print(f\"MSE: {three_comp_metrics['MSE']:.4f}\")\n",
    "print(f\"MAP@10: {three_comp_metrics['MAP@10']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings DataFrame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 25000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null userId: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null movieId: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null rating: 0\n",
      "\n",
      "Movies DataFrame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null movieId: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null title: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2540:=============================================>       (86 + 8) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null genres: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for null values in ratings_df\n",
    "print(\"Ratings DataFrame\")\n",
    "print(f\"Total rows: {ratings_df.count()}\")\n",
    "print(f\"Rows with null userId: {ratings_df.where(ratings_df.userId.isNull()).count()}\")\n",
    "print(f\"Rows with null movieId: {ratings_df.where(ratings_df.movieId.isNull()).count()}\")\n",
    "print(f\"Rows with null rating: {ratings_df.where(ratings_df.rating.isNull()).count()}\")\n",
    "\n",
    "# Check for null values in movies_df\n",
    "print(\"\\nMovies DataFrame\")\n",
    "print(f\"Total rows: {movies_df.count()}\")\n",
    "print(f\"Rows with null movieId: {movies_df.where(movies_df.movieId.isNull()).count()}\")\n",
    "print(f\"Rows with null title: {movies_df.where(movies_df.title.isNull()).count()}\")\n",
    "print(f\"Rows with null genres: {movies_df.where(movies_df.genres.isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For ratings DataFrame\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_performance(als_metrics, two_comp_metrics, three_comp_metrics):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations comparing performance of all three models:\n",
    "    1. ALS\n",
    "    2. Two-Component Hybrid (ALS + Item-CF)\n",
    "    3. Three-Component Hybrid (ALS + Item-CF + Supervised)\n",
    "    \"\"\"\n",
    "    # Set up Seaborn style for better visuals\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Prepare data in format suitable for Seaborn\n",
    "    models = ['Base ALS', 'Hybrid (ALS+CF)', 'Enhanced Hybrid']\n",
    "    metrics_data = []\n",
    "    \n",
    "    # Organize metrics data\n",
    "    for model, metrics in zip(models, [als_metrics, two_comp_metrics, three_comp_metrics]):\n",
    "        for metric_name, value in metrics.items():\n",
    "            metrics_data.append({\n",
    "                'Model': model,\n",
    "                'Metric': metric_name,\n",
    "                'Value': value\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # 1. Error Metrics Comparison Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    error_metrics = df[df['Metric'].isin(['RMSE', 'MSE'])]\n",
    "    error_plot = sns.barplot(\n",
    "        data=error_metrics,\n",
    "        x='Model',\n",
    "        y='Value',\n",
    "        hue='Metric'\n",
    "    )\n",
    "    \n",
    "    plt.title('Prediction Error Comparison', pad=20)\n",
    "    plt.ylabel('Error Value (Lower is Better)')\n",
    "    \n",
    "    # Add value labels\n",
    "    for container in error_plot.containers:\n",
    "        error_plot.bar_label(container, fmt='%.4f', padding=3)\n",
    "    \n",
    "    plt.legend(title='Error Metric')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. MAP@10 Comparison Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    map_data = df[df['Metric'] == 'MAP@10']\n",
    "    map_plot = sns.barplot(\n",
    "        data=map_data,\n",
    "        x='Model',\n",
    "        y='Value',\n",
    "        color='skyblue'\n",
    "    )\n",
    "    \n",
    "    plt.title('MAP@10 Comparison', pad=20)\n",
    "    plt.ylabel('MAP@10 Score (Higher is Better)')\n",
    "    \n",
    "    # Add value labels\n",
    "    map_plot.bar_label(map_plot.containers[0], fmt='%.4f', padding=3)\n",
    "    \n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('map_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Overall Performance Heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    heatmap_data = df.pivot(index='Model', columns='Metric', values='Value')\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt='.4f',\n",
    "        cmap='YlOrRd',\n",
    "        center=0,\n",
    "        cbar_kws={'label': 'Metric Value'}\n",
    "    )\n",
    "    \n",
    "    plt.title('Model Performance Heatmap', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Relative Improvement Plot\n",
    "    base_values = {metric: als_metrics[metric] for metric in als_metrics.keys()}\n",
    "    improvement_data = []\n",
    "    \n",
    "    for model, metrics in [(\"Hybrid\", two_comp_metrics), \n",
    "                          (\"Enhanced Hybrid\", three_comp_metrics)]:\n",
    "        for metric, value in metrics.items():\n",
    "            improvement = ((value - base_values[metric]) / base_values[metric]) * 100\n",
    "            improvement_data.append({\n",
    "                \"Model\": model,\n",
    "                \"Metric\": metric,\n",
    "                \"Improvement %\": improvement\n",
    "            })\n",
    "    \n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    improvement_plot = sns.barplot(\n",
    "        data=improvement_df,\n",
    "        x=\"Metric\",\n",
    "        y=\"Improvement %\",\n",
    "        hue=\"Model\",\n",
    "        palette=\"Set2\"\n",
    "    )\n",
    "    \n",
    "    plt.title(\"Percentage Improvement Over Base ALS Model\")\n",
    "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "    plt.xticks(rotation=30)\n",
    "    \n",
    "    # Add value labels\n",
    "    for container in improvement_plot.containers:\n",
    "        improvement_plot.bar_label(container, fmt='%.2f%%', padding=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('improvement_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Usage:\n",
    "# First ensure required imports\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create visualizations\n",
    "visualize_model_performance(als_metrics, two_comp_metrics, three_comp_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist cached DataFrames\n",
    "if 'training_data' in locals():\n",
    "    training_data.unpersist()\n",
    "if 'validation_data' in locals():\n",
    "    validation_data.unpersist()\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(als_metrics.keys(), als_metrics.values())\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.savefig('model_metrics.png')\n",
    "plt.close()\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_performance_comparison_plots_sns(als_metrics, two_comp_metrics, three_comp_metrics):\n",
    "    \"\"\"\n",
    "    Create visualizations comparing all three models using seaborn\n",
    "    \"\"\"\n",
    "    # Prepare data in long format for seaborn\n",
    "    models = ['Base ALS', 'Two-Component', 'Three-Component']\n",
    "    metrics_data = []\n",
    "    \n",
    "    # Collect data for each model\n",
    "    for model, metrics in zip(models, [als_metrics, two_comp_metrics, three_comp_metrics]):\n",
    "        for metric_name, value in metrics.items():\n",
    "            metrics_data.append({\n",
    "                'Model': model,\n",
    "                'Metric': metric_name,\n",
    "                'Value': value\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # 1. Error Metrics Plot (RMSE and MSE)\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Filter for RMSE and MSE\n",
    "    error_metrics = df[df['Metric'].isin(['RMSE', 'MSE'])]\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    error_plot = sns.barplot(\n",
    "        data=error_metrics,\n",
    "        x='Model',\n",
    "        y='Value',\n",
    "        hue='Metric'\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Error Metrics Comparison Across Models', pad=20, size=14)\n",
    "    plt.ylabel('Value (lower is better)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in error_plot.containers:\n",
    "        error_plot.bar_label(container, fmt='%.4f', padding=3)\n",
    "    \n",
    "    plt.legend(title='Metric')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_metrics_comparison_sns.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. MAP@10 Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Filter for MAP@10\n",
    "    map_data = df[df['Metric'] == 'MAP@10']\n",
    "    \n",
    "    # Create bar plot\n",
    "    map_plot = sns.barplot(\n",
    "        data=map_data,\n",
    "        x='Model',\n",
    "        y='Value',\n",
    "        palette=\"husl\"\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('MAP@10 Comparison Across Models', pad=20, size=14)\n",
    "    plt.ylabel('MAP@10 (higher is better)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    map_plot.bar_label(map_plot.containers[0], fmt='%.4f', padding=3)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('map_comparison_sns.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Heatmap of all metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = df.pivot(index='Model', columns='Metric', values='Value')\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt='.4f',\n",
    "        cmap='YlOrRd',\n",
    "        center=0,\n",
    "        cbar_kws={'label': 'Value'}\n",
    "    )\n",
    "    \n",
    "    plt.title('Performance Metrics Heatmap', pad=20, size=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_heatmap_sns.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Point plot with confidence intervals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    sns.pointplot(\n",
    "        data=df,\n",
    "        x='Model',\n",
    "        y='Value',\n",
    "        hue='Metric',\n",
    "        dodge=True,\n",
    "        markers=['o', 's', 'D'],\n",
    "        linestyles=['-', '--', ':']\n",
    "    )\n",
    "    \n",
    "    plt.title('Performance Metrics with Confidence Intervals', pad=20, size=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_pointplot_sns.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations with actual metrics\n",
    "create_performance_comparison_plots_sns(als_metrics, two_comp_metrics, three_comp_metrics)\n",
    "def radar_comparison(als_metrics, two_comp_metrics, three_comp_metrics):\n",
    "    \"\"\"\n",
    "    Create radar chart and comprehensive model comparison visualizations\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # 1. Comprehensive Model Comparison\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Set up data\n",
    "    models = ['Base ALS', 'Two-Component', 'Three-Component']\n",
    "    metrics = {\n",
    "        'RMSE': [als_metrics['RMSE'], two_comp_metrics['RMSE'], three_comp_metrics['RMSE']],\n",
    "        'MSE': [als_metrics['MSE'], two_comp_metrics['MSE'], three_comp_metrics['MSE']],\n",
    "        'MAP@10': [als_metrics['MAP@10'], two_comp_metrics['MAP@10'], three_comp_metrics['MAP@10']]\n",
    "    }\n",
    "    \n",
    "    # Position of bars on x-axis\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25  # Width of bars\n",
    "    \n",
    "    # Create grouped bars\n",
    "    plt.bar(x - width, metrics['RMSE'], width, label='RMSE', color='#2ecc71')\n",
    "    plt.bar(x, metrics['MSE'], width, label='MSE', color='#3498db')\n",
    "    plt.bar(x + width, metrics['MAP@10'], width, label='MAP@10', color='#9b59b6')\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Metric Values')\n",
    "    plt.title('Comprehensive Model Comparison')\n",
    "    plt.xticks(x, models)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i in range(len(models)):\n",
    "        plt.text(i - width, metrics['RMSE'][i], f'{metrics[\"RMSE\"][i]:.4f}', \n",
    "                ha='center', va='bottom')\n",
    "        plt.text(i, metrics['MSE'][i], f'{metrics[\"MSE\"][i]:.4f}', \n",
    "                ha='center', va='bottom')\n",
    "        plt.text(i + width, metrics['MAP@10'][i], f'{metrics[\"MAP@10\"][i]:.4f}', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Radar Chart\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Normalize metrics for radar chart\n",
    "    max_rmse = max(metrics['RMSE'])\n",
    "    max_mse = max(metrics['MSE'])\n",
    "    max_map = max(metrics['MAP@10'])\n",
    "    \n",
    "    # Invert RMSE and MSE because lower is better\n",
    "    normalized_metrics = {\n",
    "        'Base ALS': [1 - (als_metrics['RMSE']/max_rmse),\n",
    "                    1 - (als_metrics['MSE']/max_mse),\n",
    "                    als_metrics['MAP@10']/max_map],\n",
    "        'Two-Component': [1 - (two_comp_metrics['RMSE']/max_rmse),\n",
    "                         1 - (two_comp_metrics['MSE']/max_mse),\n",
    "                         two_comp_metrics['MAP@10']/max_map],\n",
    "        'Three-Component': [1 - (three_comp_metrics['RMSE']/max_rmse),\n",
    "                          1 - (three_comp_metrics['MSE']/max_mse),\n",
    "                          three_comp_metrics['MAP@10']/max_map]\n",
    "    }\n",
    "    \n",
    "    # Set up the angles of the radar chart\n",
    "    labels = ['RMSE\\n(inverted)', 'MSE\\n(inverted)', 'MAP@10']\n",
    "    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # complete the circle\n",
    "    \n",
    "    # Plot for each model\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    colors = ['#2ecc71', '#3498db', '#9b59b6']\n",
    "    \n",
    "    for model, color in zip(normalized_metrics.keys(), colors):\n",
    "        values = normalized_metrics[model]\n",
    "        values += values[:1]  # complete the circle\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=model, color=color)\n",
    "        ax.fill(angles, values, alpha=0.25, color=color)\n",
    "    \n",
    "    # Set the labels and title\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    plt.title('Relative Performance Across Metrics', pad=20)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Usage example:\n",
    "radar_comparison(als_metrics, two_comp_metrics, three_comp_metrics)\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def additional_insights(als_metrics, two_comp_metrics, three_comp_metrics):\n",
    "    \"\"\"Create additional visualizations for deeper insights\"\"\"\n",
    "    \n",
    "    # 1. Performance Distribution Plot\n",
    "    # Prepare data\n",
    "    data = []\n",
    "    for model, metrics in [(\"Base ALS\", als_metrics), \n",
    "                          (\"Two-Component\", two_comp_metrics), \n",
    "                          (\"Three-Component\", three_comp_metrics)]:\n",
    "        for metric, value in metrics.items():\n",
    "            data.append({\n",
    "                \"Model\": model,\n",
    "                \"Metric\": metric,\n",
    "                \"Value\": value,\n",
    "                \"Normalized Value\": (value - min(als_metrics[metric], \n",
    "                                               two_comp_metrics[metric], \n",
    "                                               three_comp_metrics[metric])) / \n",
    "                                  (max(als_metrics[metric], \n",
    "                                      two_comp_metrics[metric], \n",
    "                                      three_comp_metrics[metric]) - \n",
    "                                   min(als_metrics[metric], \n",
    "                                       two_comp_metrics[metric], \n",
    "                                       three_comp_metrics[metric]))\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create violin plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(data=df, x=\"Model\", y=\"Normalized Value\", hue=\"Metric\")\n",
    "    plt.title(\"Distribution of Normalized Metrics Across Models\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metric_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Correlation Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = pd.pivot_table(df, \n",
    "                                      values='Value', \n",
    "                                      index='Model', \n",
    "                                      columns='Metric').corr()\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0, \n",
    "                fmt='.2f')\n",
    "    plt.title(\"Correlation Between Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metric_correlation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Relative Improvement Plot\n",
    "    # Calculate improvement percentages relative to base ALS\n",
    "    improvement_data = []\n",
    "    base_values = {metric: als_metrics[metric] for metric in als_metrics.keys()}\n",
    "    \n",
    "    for model, metrics in [(\"Two-Component\", two_comp_metrics), \n",
    "                          (\"Three-Component\", three_comp_metrics)]:\n",
    "        for metric, value in metrics.items():\n",
    "            improvement = ((value - base_values[metric]) / base_values[metric]) * 100\n",
    "            improvement_data.append({\n",
    "                \"Model\": model,\n",
    "                \"Metric\": metric,\n",
    "                \"Improvement %\": improvement\n",
    "            })\n",
    "    \n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=improvement_df, \n",
    "                x=\"Metric\", \n",
    "                y=\"Improvement %\", \n",
    "                hue=\"Model\", \n",
    "                palette=\"Set2\")\n",
    "    plt.title(\"Percentage Improvement Over Base ALS Model\")\n",
    "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('improvement_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Performance Trade-off Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df[df['Metric'].isin(['RMSE', 'MAP@10'])],\n",
    "                    x='Value',\n",
    "                    y='Normalized Value',\n",
    "                    hue='Model',\n",
    "                    style='Metric',\n",
    "                    s=100)\n",
    "    plt.title(\"Performance Trade-offs Between RMSE and MAP@10\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tradeoff_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Example usage:\n",
    "additional_insights(als_metrics, two_comp_metrics, three_comp_metrics)\n",
    "# Unpersist cached DataFrames\n",
    "if 'training_data' in locals():\n",
    "    training_data.unpersist()\n",
    "if 'validation_data' in locals():\n",
    "    validation_data.unpersist()\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
